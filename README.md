# Лабораторная работа: Обучение перцептрона 

## Цель

Реализовать и обучить однослойный перцептрон на синтетических данных. Выполнить нормализацию признаков, оценить точность модели.

---

## Используемые библиотеки

| Библиотека         | Назначение                                                         |
|--------------------|--------------------------------------------------------------------|
| `numpy`            | Векторные и матричные вычисления                                   |
| `pandas`           | Работа с таблицами и сохранение датасета в CSV                    |
| `matplotlib.pyplot`| Построение графиков (ошибка и веса)                               |
| `sklearn.datasets` | Генерация синтетического классификационного датасета              |
| `sklearn.preprocessing` | Нормализация данных (стандартизация признаков)            |

---

## Алгоритм: Перцептрон

Модель представляет собой **однослойный перцептрон** с пороговой функцией активации:

- Весовые коэффициенты инициализируются случайно
- Обновление весов выполняется по правилу: w = w + learning_rate * (d - y) * x
- Функция активации: activation(z) = 1, если z >= 0, иначе activation(z) = 0

### `__init__(self, input_size, learning_rate=0.01, epochs=200)`
- Инициализирует веса случайными значениями в диапазоне `[-0.5, 0.5]`.
- Добавляется дополнительный вес для **bias (смещения)**.
- Принимает параметры обучения: скорость (`learning_rate`) и количество эпох (`epochs`).

### `activation(self, z)`
- Пороговая функция активации:
  - `1`, если `z >= 0`
  - `0`, если `z < 0`

### `predict(self, x)`
- Добавляет `bias` к входному вектору.
- Вычисляет взвешенную сумму и применяет активацию.
- Возвращает предсказанный класс (0 или 1).

### `train(self, X, d)`
- Обучает модель на множестве входов `X` и целевых метках `d`.
- На каждой эпохе:
  - Сравнивает предсказание с меткой.
  - Вычисляет ошибку и обновляет веса.
- Каждые 20 эпох выводит промежуточную ошибку в консоль.

---

## Источники используемых функций

- `np.insert` — добавление bias (единичного элемента) в начало вектора.
- `np.dot` — скалярное произведение весов и входов.
- `np.random.rand(...) - 0.5` — генерация начальных весов.
- `StandardScaler` — нормализация данных до нулевого среднего и единичного отклонения.
  
---

## Этапы работы

### 1. Генерация данных

С помощью `make_classification` создаётся 300 примеров с 10 признаками, 5 из которых — информативны.

### 2. Нормализация

Признаки стандартизируются с помощью `StandardScaler` для улучшения сходимости обучения.

### 3. Обучение

Параметры модели:
- `learning_rate = 0.05`
- `epochs = 800`

### 4. Оценка точности

После обучения модель тестируется на данных, и выводится доля правильных предсказаний.

---

## Визуализация

### Ошибка по эпохам

Показывает, как уменьшается количество ошибок в процессе обучения:

![image](https://github.com/user-attachments/assets/2763da0e-50a8-42c5-82cb-359d50aad796)


### Веса модели

Позволяет понять, какие признаки оказали большее влияние на классификацию:

![image](https://github.com/user-attachments/assets/b827f455-f729-4939-98a2-86827c91a16c)


---

## Результаты

- Точность на тестовой выборке: **~75-85%**
- Ошибка обучения постепенно снижается
- Наиболее значимые признаки получают более высокие веса

---

## Сохранение данных

Сгенерированный и нормализованный датасет сохраняется в файл: 'large_dataset.csv'


